{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e78fedfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import string\n",
    "import collections\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6031a385",
   "metadata": {},
   "source": [
    "# yelp reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2f1a92be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    frequency_cutoff=25,\n",
    "    model_state_file='model.pth',\n",
    "    review_csv='yelp_small.csv',\n",
    "    save_dir='model_storage/yelp/',\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=100,\n",
    "    seed=1337,\n",
    "    train_proportion=80,\n",
    "    val_proportion=10,\n",
    "    test_proportion=10,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f1e2b202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text\n",
    "\n",
    "def split_data(review_subset):\n",
    "    by_rating = collections.defaultdict(list)\n",
    "    for _, row in review_subset.iterrows():\n",
    "        by_rating[row.stars].append(row.to_dict())\n",
    "\n",
    "    final_list = []\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    for _, item_list in sorted(by_rating.items()):\n",
    "        np.random.shuffle(item_list)\n",
    "\n",
    "        n_total = len(item_list)\n",
    "        n_train = int(args.train_proportion * n_total)\n",
    "        n_val = int(args.val_proportion * n_total)\n",
    "        n_test = int(args.test_proportion * n_total)\n",
    "\n",
    "        for item in item_list[:n_train]:\n",
    "            item['split'] = 'train'\n",
    "        for item in item_list[n_train:n_train+n_val]:\n",
    "            item['split'] = 'val'\n",
    "        for item in item_list[n_train+n_val:n_train+n_val+n_test]:\n",
    "            item['split'] = 'test'\n",
    "\n",
    "        final_list.extend(item_list)\n",
    "\n",
    "    final_reviews = pd.DataFrame(final_list)\n",
    "    final_reviews.text = final_reviews.text.apply(preprocess_text)\n",
    "    return final_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f9ba4912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, review_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            review_df (pendas.DataFrame) the dataset\n",
    "            vectorizer (reviewVectorizer) vectorizer instantiated from dataset\n",
    "        \"\"\"\n",
    "        self.review_df = split_data(review_df)\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        self.train_df = self.review_df[self.review_df.split==\"train\"]\n",
    "        self.train_size = len(self.train_df)\n",
    "        \n",
    "        self.val_df = self.review_df[self.review_df.split==\"val\"]\n",
    "        self.validation_size = len(self.val_df)\n",
    "        \n",
    "        self.test_df = self.review_df[self.review_df.split==\"test\"]\n",
    "        self.test_size = len(self.test_df)\n",
    "        \n",
    "        self._lookup_dict = {\"train\": (self.train_df, self.train_size),\n",
    "                             \"val\": (self.val_df, self.validation_size),\n",
    "                             \"test\": (self.test_df, self.test_size)}\n",
    "        \n",
    "        self.set_split('train')\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, review_csv):\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        return cls(review_df, ReviewVectorizer.from_dataframe(review_df))\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "    \n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        \n",
    "        review_vector = self._vectorizer.vectorize(row.text)\n",
    "        \n",
    "        rating_index = self._vectorizer.rating_vocab.lookup_token(row.stars)\n",
    "        \n",
    "        return {'x_data': review_vector,\n",
    "                'y_target': rating_index}\n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5b81c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract Vocabulary for mapping\"\"\"\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self.unk_token = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self.token_to_idx, 'add_unk': self._add_unk,\n",
    "                'unk_token': self._unk_token}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, content):\n",
    "        return cls(**content)\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        if self._add_unk:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)\" % len(self)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3f644584",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewVectorizer(object):\n",
    "    def __init__(self, review_vocab, rating_vocab):\n",
    "        self.review_vocab = review_vocab\n",
    "        self.rating_vocab = rating_vocab\n",
    "        \n",
    "    def vectorize(self, review):\n",
    "        one_hot = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
    "        for token in review.split(\" \"):\n",
    "            if token not in string.punctuation:\n",
    "                one_hot[self.review_vocab.lookup_token(token)] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, review_df, cutoff=25):\n",
    "        review_vocab = Vocabulary(add_unk=True)\n",
    "        rating_vocab = Vocabulary(add_unk=False)\n",
    "        \n",
    "        for rating in sorted(set(review_df.stars)):\n",
    "            rating_vocab.add_token(rating)\n",
    "            \n",
    "        word_counts = Counter()\n",
    "        for review in review_df.text:\n",
    "            for word in review.split(\" \"):\n",
    "                if word not in string.punctuation:\n",
    "                    word_counts[word] += 1\n",
    "        for word, count in word_counts.items():\n",
    "            if count > cutoff:\n",
    "                review_vocab.add_token(word)\n",
    "                \n",
    "        return cls(review_vocab, rating_vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        review_vocab = Vocabulary.from_serializable(contents['review_vocab'])\n",
    "        rating_vocab = Vocabulary.from_serializable(contents['rating_vocab'])\n",
    "        \n",
    "        return cls(review_vocab=review_vocab, rating_vocab=rating_vocab)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'review_vocab': self.review_vocab.to_serializable(), \n",
    "                'rating_vocab': self.rating_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3749fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                           shuffle=False, drop_last=drop_last)\n",
    "    \n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "05a00e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ReviewClassifier(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=num_features, out_features=1)\n",
    "        \n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "        y_out = self.fc1(x_in).squeeze()\n",
    "        if apply_sigmoid:\n",
    "            y_out = F.sigmoid(y_out)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b744c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def make_train_state(args):\n",
    "    return {'epoch_index': 0, \n",
    "           'train_loss': [],\n",
    "            'train_acc': [],\n",
    "           'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1}\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "args.device = \"cpu\"\n",
    "\n",
    "dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = ReviewClassifier(num_features=len(vectorizer.review_vocab))\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "23c8b8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "for epoch_index in range(args.num_epochs):\n",
    "    train_state['epoch_index'] = epoch_index\n",
    "    \n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_batches(dataset, batch_size=args.batch_size,)\n",
    "    running_loss=0.0\n",
    "    running_acc =0.0\n",
    "    classifier.train()\n",
    "    \n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "        \n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        #acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        #running_acc (acc_batch - running_acc) / (batch_index + 1)\n",
    "    \n",
    "    #train_state['train_loss'].append(running_loss)\n",
    "    #train_state['train_acc'].append(running_acc)\n",
    "    \n",
    "    dataset.set_split(\"val\")\n",
    "    batch_generator = generate_batches(dataset, batch_size=args.batch_size,)\n",
    "    running_loss=0.0\n",
    "    running_acc =0.0\n",
    "    classifier.eval()\n",
    "    \n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "        \n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_batch = loss.item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "        \n",
    "        \n",
    "        acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc = (acc_batch - running_acc) / (batch_index + 1)\n",
    "    #train_state['val_loss'].append(running_loss)\n",
    "    #train_state['val_acc'].append(running_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0576eaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x_data': tensor([[1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]]), 'y_target': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'x_data': tensor([[1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.]]), 'y_target': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'x_data': tensor([[1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]]), 'y_target': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2])}\n",
      "{'x_data': tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.]]), 'y_target': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2])}\n",
      "{'x_data': tensor([[1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.]]), 'y_target': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2])}\n",
      "{'x_data': tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.]]), 'y_target': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3])}\n",
      "{'x_data': tensor([[1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]]), 'y_target': tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3])}\n",
      "{'x_data': tensor([[1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]]), 'y_target': tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3])}\n",
      "{'x_data': tensor([[1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.]]), 'y_target': tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3])}\n",
      "{'x_data': tensor([[1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.]]), 'y_target': tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4])}\n",
      "{'x_data': tensor([[1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]]), 'y_target': tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4])}\n",
      "{'x_data': tensor([[1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]]), 'y_target': tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4])}\n"
     ]
    }
   ],
   "source": [
    "dataset.set_split('train')\n",
    "batch_generator = generate_batches(dataset, batch_size=args.batch_size,)\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    print(batch_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c7ab3ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(review, classifier, vectorizer, decision_threshold=0.5):\n",
    "    \"\"\"\n",
    "    review: text to review\n",
    "    classifier: trained model,\n",
    "    \n",
    "    \"\"\"\n",
    "    review = preprocess_text(review)\n",
    "    vectorized_review = torch.tensor(vectorizer.vectorize(review))\n",
    "    result =classifier (vectorized_review.view(1, -1))\n",
    "    \n",
    "    probability_value = F.sigmoid(result).item()\n",
    "    \n",
    "    index = 1\n",
    "    if probability_value < decision_threshold:\n",
    "        index = 0\n",
    "    \n",
    "    return vectorizer.rating_vocab.lookup_index(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b30851f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is so bad, I hate it -> 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gnu/store/ziasw222h44mn40v4r43l3k4rgpmbcsx-profile/lib/python3.9/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "test_review = \"this is so bad, I hate it\"\n",
    "prediction = predict_rating(test_review, classifier, vectorizer)\n",
    "print(\"{} -> {}\".format(test_review, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6e2c8967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is so good, I like the ambiance and the nice tables -> 2\n"
     ]
    }
   ],
   "source": [
    "test_review = \"this is so good, I like the ambiance and the nice tables\"\n",
    "prediction = predict_rating(test_review, classifier, vectorizer)\n",
    "print(\"{} -> {}\".format(test_review, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "905b1f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " most best:\n",
      "meal\n",
      "fan\n",
      "typical\n",
      "spot\n",
      "although\n",
      "liked\n",
      "chips\n",
      "salad\n",
      "little\n",
      "pretty\n",
      "aussi\n",
      "packed\n",
      "though\n",
      "ambiance\n",
      "side\n",
      "menu\n",
      "quite\n",
      "tables\n",
      "nice\n",
      "sauce\n"
     ]
    }
   ],
   "source": [
    "fc1_weights = classifier.fc1.weight.detach()[0]\n",
    "_, indices = torch.sort(fc1_weights, dim=0, descending=True)\n",
    "indices = indices.numpy().tolist()\n",
    "\n",
    "print(\" most best:\")\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c88429ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad\n",
      "well,\n",
      "On\n",
      "place!\n",
      "delicious.\n",
      "\n",
      "\n",
      "This\n",
      "go.\n",
      "menu,\n",
      "cheese.\n",
      "in,\n",
      "2\n",
      "I'll\n",
      "Our\n",
      "now,\n",
      "tr√®s\n",
      "Les\n",
      "Un\n",
      "you.\n",
      "Il\n",
      "it,\n",
      "couldn't\n"
     ]
    }
   ],
   "source": [
    "print(\"bad\")\n",
    "indices.reverse()\n",
    "for i in range(20):\n",
    "    print(vectorizer.review_vocab.lookup_index(indices[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b8ec41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "/gnu/store/bbakd2h7s4g62fymjmqpxgvlanp1ydny-python-3.9.9/bin/python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
